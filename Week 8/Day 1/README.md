<p align = "center" draggable=â€falseâ€ ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>

## <h1 align="center" id="heading">RAG In Production: Buildin' them Apps</h1>

### Steps to Run:

1. Create a Python 3.9 environment
2. `pip install -r requirements.txt`
3. `pip install jupyter`
3. Run through the notebook. 

# Build ðŸ—ï¸

Run the notebook ([Colab link](https://colab.research.google.com/drive/1YsYbNpH7VrqAGdv8aBWGuRDu8biknL_w?usp=sharing)) and complete the following:

1. ðŸ¤ BREAKOUT ROOM #1:
  - Task 1: Depends and Set-Up
  - Task 2: Setting up RAG With Production in Mind
  - Task 3: RAG LCEL Chain
2. ðŸ¤ BREAKOUT ROOM #2:
  - Deploy HF Application Using Resources in `Application Start` folder. 
    - `app.py` skeleton is provided
    - `requirements.txt` is provided
    - `Dockerfile` is provided

## ADVANCED BUILD:

The caching we're using is both: 

1. Ineffecient
2. Exact Match

Please produce a locally running application (through Docker) that integrates a more intelligent caching process.

In simpler terms: 

- Use a database approach (Redis, Vectordatase, SQLite, etc.) instead of plain-memory for caching
- Implement Semantic LLM Caching OR Implement E2E Caching

> NOTE: Doing the advanced build will count as your assignment for the week. If you do the advanced build, you are not required to do the notebook.

# Ship ðŸš¢

- HF App.
- 5min. Loom Video

# Share ðŸš€
- Walk through your notebook and explain what you've completed in the Loom video
- Make a social media post about your final application and tag @AIMakerspace
- Share 3 lessons learned
- Share 3 lessons not learned
