{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "\n",
        "  - ü§ù Breakout Room #2:\n",
        "  1. Evaluating the LangGraph Application with LangSmith\n",
        "  2. Adding Helpfulness Check and \"Loop\" Limits\n",
        "  3. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KaVwN269EttM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# !pip install -qU langchain langchain-openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1\n",
        "!pip install -qU duckduckgo_search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "42b2ba5e-11ae-4f4d-e68e-77607bb794ad"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "30aa2260-50f1-4abf-b305-eed0ee1b9008"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE4 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1: - `COMPLETED BELOW!!!`\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    ### YOUR CODE HERE‚ò∫\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "model = model.bind_tools(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANSWER #1\n",
        "\n",
        "The model uses the `description` attribute of each tool.  The `description` is a string that should provide a clear explanation of what the tool does.  This `description` string is passed to the LLM to allow it to decide which tool to use.\n",
        "\n",
        "For custom user-defined functions to be used as tools, use the @tool decorator and provide a `docstring` for the function to be used as a tool.  The @tool decorator will use the function's `dosctring` as the `description`, so a `docstring` must be provided!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolNode\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "tool_node = ToolNode(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `tool_node` is a node which can call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "uncompiled_graph = StateGraph(AgentState)\n",
        "\n",
        "uncompiled_graph.add_node(\"agent\", call_model)\n",
        "uncompiled_graph.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  return END\n",
        "\n",
        "uncompiled_graph.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "uncompiled_graph.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "compiled_graph = uncompiled_graph.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANSWER #2:\n",
        "\n",
        "In the way the graph is set up above, there is no specific limit to the number of times we can cycle through the nodes.  As it is currently written, the only time the code will end is if the AIMessage that is appended to the AgentState in the agent node does not have a `tool_calls` kwarg set.  And as it is currently written this implies a non-deterministic flow through the graph.\n",
        "\n",
        "We can impose a limit to number of cycles by augmenting the definition of the state to include a counter on the number of times that the code has cycled through the graph.  For example, the counter can increment each time the `agent` node is traversed.  If the number of loops exceeds a maximum count, which can be set as a parameter, then we can include this additional exit condition in the should_continue function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_tolps3eP5QyhRd8dZpdg7SYs', 'function': {'arguments': '{\"query\":\"current captain of the Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 156, 'total_tokens': 181}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-206c3410-d82c-46cf-89b0-b13da099791c-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of the Winnipeg Jets 2023'}, 'id': 'call_tolps3eP5QyhRd8dZpdg7SYs', 'type': 'tool_call'}], usage_metadata={'input_tokens': 156, 'output_tokens': 25, 'total_tokens': 181})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='September 12, 2023. There are not many honours in team sports bigger than being named captain. That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the ... - Sep 12, 2023 After a season without a captain, the Winnipeg Jets have named their new leader. Centre Adam Lowry, who is about to enter his 10th season as a Jet, was given the honour Tuesday. Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club\\'s new captain on Tuesday morning. \"When I ... The Winnipeg Jets have named Adam Lowry the third captain in franchise history. Mike Stephens Sep 12, 2023 Adam Lowry James Carey Lauder-USA TODAY Sports', name='duckduckgo_search', tool_call_id='call_tolps3eP5QyhRd8dZpdg7SYs')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='As of September 12, 2023, the current captain of the Winnipeg Jets is Adam Lowry.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 431, 'total_tokens': 454}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-66f661fb-fffd-4a58-836c-d63d4526a1b5-0', usage_metadata={'input_tokens': 431, 'output_tokens': 23, 'total_tokens': 454})]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the Winnipeg Jets?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MY TRIAL #1: Captains of All Canadian NHL Teams\n",
        "\n",
        "To experiment with multiple and possibly parallel calls to the Tools in ToolBelt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "user_query = \"Who is the current captain of the team that plays ice hockey in the NHL in Canada?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "a5d7ef7a-13f2-4066-df52-b0df89eae2ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_mi4RDbVVvGoHOPZ5W1uHi9lP', 'function': {'arguments': '{\"query\":\"current captain of NHL teams in Canada 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 164, 'total_tokens': 190}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-35dd1364-a1ad-4152-9daa-8f8836f1cfc9-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of NHL teams in Canada 2023'}, 'id': 'call_mi4RDbVVvGoHOPZ5W1uHi9lP', 'type': 'tool_call'}], usage_metadata={'input_tokens': 164, 'output_tokens': 26, 'total_tokens': 190})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='The 2023-24 NHL season is finally here, and for some players, they\\'ll be entering the year with the honored symbol of leadership on their jersey: A \"C\" for captain. Who are the current NHL team captains? 2024-25 season list. Brad Marchand was named captain of the Boston Bruins prior to the start of the 2023-24 NHL season. can vary by team, and there are ... The 2023-24 season is Crosby\\'s 17th as captain, so the only question now is if he ties, or beats, Yzerman\\'s record for most seasons as a team\\'s captain. 24 of 32 San Jose Sharks : Logan Couture Who is each NHL team\\'s captain? Current list for 2023-24. 75d ESPN staff. NHL playoff watch: Stars, Avalanche face off in key clash in Central race. Who is each NHL team\\'s captain? Current list for 2023-24. 34d ESPN staff. McDavid, then who? The NHL\\'s best centers right now, according to players and execs.', name='duckduckgo_search', tool_call_id='call_mi4RDbVVvGoHOPZ5W1uHi9lP')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_0ji0TcP2fMWDKfXadIqb1dNj', 'function': {'arguments': '{\"query\":\"current captain of Canadian NHL teams 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 423, 'total_tokens': 448}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-29d7b6a5-4d85-45ba-8434-3f2f99d0d771-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of Canadian NHL teams 2023'}, 'id': 'call_0ji0TcP2fMWDKfXadIqb1dNj', 'type': 'tool_call'}], usage_metadata={'input_tokens': 423, 'output_tokens': 25, 'total_tokens': 448})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='The 2023-24 NHL season is finally here, and for some players, they\\'ll be entering the year with the honored symbol of leadership on their jersey: A \"C\" for captain. Not every team has a ... Who are the current NHL team captains? 2024-25 season list. Brad Marchand was named captain of the Boston Bruins prior to the start of the 2023-24 NHL season. can vary by team, and there are ... The last team on the list, and also one more team with a new captain for the 2023-24 season. Lowry is an odd choice, but current Jets head coach Rick Bowness is an odd guy, so that tracks. Every captain for 2023-24 NHL season as Brad Marchand and Quinn Hughes add the \\'C\\' The Vancouver Canucks and Boston Bruins are two of the latest teams to appoint new captains as Quinn Hughes and Brad Marchand will now have a \"C\" stitched onto their jerseys. After five teams anointed new players to wear the \"C\" this season, 27 of 32 clubs have a player with that designation. Here is the full list. ... Who is each NHL team\\'s captain? Current list for 2023-24. 44d ESPN staff. NHL trade grades megafile: Report cards for Wennberg, Henrique, Tarasenko, more deals. Montreal Canadiens.', name='duckduckgo_search', tool_call_id='call_0ji0TcP2fMWDKfXadIqb1dNj')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_VfthOLtqepoLB5jXICOCOXEI', 'function': {'arguments': '{\"query\": \"current captain of Toronto Maple Leafs 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_SCkEBlsYuUyuj3ReSzbMjp7f', 'function': {'arguments': '{\"query\": \"current captain of Montreal Canadiens 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_NtFOxb1wdDhB6t1urDuboGdW', 'function': {'arguments': '{\"query\": \"current captain of Vancouver Canucks 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_zX39Ty01N9OkYe4YjQ4Afzrx', 'function': {'arguments': '{\"query\": \"current captain of Calgary Flames 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_nJf7KLPeF8x0XT0ACL2Ib3YC', 'function': {'arguments': '{\"query\": \"current captain of Edmonton Oilers 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_XW5RcQV79qUvW8iQlHcacQ4r', 'function': {'arguments': '{\"query\": \"current captain of Winnipeg Jets 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_OxSMVguUNBhUP5eCSfGawitd', 'function': {'arguments': '{\"query\": \"current captain of Ottawa Senators 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 744, 'total_tokens': 931}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d859eba5-62af-4982-9492-661679e3bdb5-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captain of Toronto Maple Leafs 2023'}, 'id': 'call_VfthOLtqepoLB5jXICOCOXEI', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'current captain of Montreal Canadiens 2023'}, 'id': 'call_SCkEBlsYuUyuj3ReSzbMjp7f', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'current captain of Vancouver Canucks 2023'}, 'id': 'call_NtFOxb1wdDhB6t1urDuboGdW', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'current captain of Calgary Flames 2023'}, 'id': 'call_zX39Ty01N9OkYe4YjQ4Afzrx', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'current captain of Edmonton Oilers 2023'}, 'id': 'call_nJf7KLPeF8x0XT0ACL2Ib3YC', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'current captain of Winnipeg Jets 2023'}, 'id': 'call_XW5RcQV79qUvW8iQlHcacQ4r', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'current captain of Ottawa Senators 2023'}, 'id': 'call_OxSMVguUNBhUP5eCSfGawitd', 'type': 'tool_call'}], usage_metadata={'input_tokens': 744, 'output_tokens': 187, 'total_tokens': 931})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"The 26-year-old forward led all NHL skaters in combined posts and crossbars hit during the 2023-24 season. ... He was also named the 26th captain in Maple Leafs history on Aug. 14, taking the ... The Toronto Maple Leafs formally announced Auston Matthews as the 26th captain in team history on Wednesday, with now-former captain John Tavares expressing his unequivocal support in passing the torch to one of the league's superstars.. The Maple Leafs made the announcement at a press conference in Toronto, where Matthews ‚Äî the first American to be named team captain ‚Äî spoke alongside ... Toronto Maple Leafs center Auston Matthews (34) scores a goal and celebrates with Toronto Maple Leafs captain John Tavares (91) Nick Turchiaro-USA TODAY Sports. Tavares is entering the final year ... Toronto Maple Leafs' Auston Matthews is handed his captain's jersey from former captain John Tavares during an announcement for Matthews' captaincy in Toronto, Wednesday, Aug. 14, 2024. THE ... The Toronto Maple Leafs passed the captaincy torch from John Tavares onto Auston Matthews who will become the Blue and White's 26th captain in team history. in Toronto on Wednesday August 14, 2024 ...\", name='duckduckgo_search', tool_call_id='call_VfthOLtqepoLB5jXICOCOXEI'), ToolMessage(content=\"In 2023-24, and for a third year in a row, the captain won the Molson Cup with the Canadiens. He's still a long way from Guy Lafleur's six Molson Cups in a row, but he's catching up to Carey ... Nick Suzuki's 2023-24 season with the Montreal Canadiens was marked by significant achievements both on and off the ice, solidifying his role as captain. His key moments included advocating for line changes that boosted team performance, reaching the 100-goal milestone, and winning the Molson Cup for the third consecutive year. BROSSARD - Montreal Canadiens forward Nick Suzuki has been named the winner of the Molson Cup Player of the Year for the 2023-24 season. The captain, who won the monthly award in February, is now ... Categories Montreal Canadiens Tags 2024-25 NHL Season Preview, Kirby Dach, Montreal Canadiens Season Preview, Patrik Laine Top 10 Draft-Eligible WHL Players for 2024-25: Preseason Edition Montreal's expected goals against per 60 minutes (xGA/60) at five-on-five has barely budged, moving from a 28th-ranked 2.98 in St-Louis' first year in charge to a 30th-ranked 2.89 last season.\", name='duckduckgo_search', tool_call_id='call_SCkEBlsYuUyuj3ReSzbMjp7f'), ToolMessage(content=\"Defenseman Quinn Hughes is the new captain of the Vancouver Canucks, the team announced Monday. Here's what you need to know: Hughes is the 15th captain in franchise history. September 11, 2023. Quinn Hughes was named captain of the Vancouver Canucks on Monday. The 23-year-old is the 15th captain and third defenseman to hold the title in Canucks history. Kevin McCarthy ... Tyson Cole - @SpittinPicklets The 2023-2024 season has been an eventful one for Quinn Hughes. On opening night, Hughes was honoured as the 15th captain in Vancouver Canucks history. The seventh ... Vancouver Canucks name Quinn Hughes as captain Vancouver Canucks' Quinn Hughes, centre, addresses the fans after defeating the Calgary Flames in their final NHL hockey home game of the year, in Vancouver, on Saturday, April 8, 2023. Vancouver Canucks' Quinn Hughes, middle, wears his new captain jersey as Canucks GM Patrik Allvin, left, and head coach Rick Tocchet, right, look on at a news conference in Vancouver on Monday.\", name='duckduckgo_search', tool_call_id='call_NtFOxb1wdDhB6t1urDuboGdW'), ToolMessage(content=\"Forward has played entire 908-game career in Calgary. September 27, 2023. The Calgary Flames announced today the signing of Mikael Backlund to a two-year contact extension. Additionally, Backlund ... Calgary, AB - The Calgary Flames announced today the signing of Mikael Backlund to a two-year contact extension. Additionally, Backlund has been named the 21 st captain in Flames franchise history. ... Mikael Backlund was named the 2023 King Clancy Memorial Trophy winner for his humanitarian efforts throughout the 2022-23 season, becoming the ... CALGARY, Alberta (AP) ‚Äî Mikael Backlund signed a two-year, $9-million extension with the Calgary Flames and was named captain, the team announced Wednesday. Backlund would have become an unrestricted free agent next summer without an extension. The 34-year-old center from Sweden posted a career year in 2022-23 with 56 points (19 goals, 37 ... The Calgary Flames have finally named the 21st captain in franchise history. ... and it's time to look at why he was named captain and how this news shapes the rest of the Flames' 2023-24 ... Here's how dialogue on the next Calgary Flames captain has progressed. The Calgary Flames' captain dilemma timeline. Back in May of this year, Flames General Manager, Craig Conroy, joined Pat Steinberg on Flames Talk and wasn't shy to share that a captain would be named in advance of the 2023/24 season (which is now only 26 days away).\", name='duckduckgo_search', tool_call_id='call_zX39Ty01N9OkYe4YjQ4Afzrx'), ToolMessage(content=\"2023-24 Edmonton Oilers Roster and Statistics. 2023-24. Edmonton Oilers. Roster and Statistics. Record: 49-27-6 (104 points), Finished 2nd in NHL Pacific Division ( Schedule and Results ) Coach: Jay Woodcroft (3-9-1), Kris Knoblauch (46-18-5) Captain: Connor McDavid. Primary Arena: Rogers Place. The 2023-24 Edmonton Oilers season was the 45th season for the National Hockey League (NHL) franchise that was established on June 22, 1979, and 52nd season for the organization overall, including their play in the World Hockey Association (WHA).. During the offseason, the Oilers acquired forward Connor Brown and defenceman Ben Gleason via free agency. [1] [2] They also re-signed forward ... In 2022-23, he recorded his 150 th point in Edmonton's 80 th game, and the Oilers will play game No. 80 on April 15, against the San Jose Sharks at Rogers Place. The 2022-23 Edmonton Oilers season was the 44th season for the National Hockey League (NHL) franchise that was established on June 22, 1979, and 51st season for the organization overall, including their play in the World Hockey Association (WHA).. On April 1, 2023, the Oilers clinched a playoff berth after a win over the Anaheim Ducks. [2]The Oilers became the first NHL team since the 1995 ... Connor Andrew McDavid (born January 13, 1997) is a Canadian professional ice hockey centre and captain of the Edmonton Oilers of the National Hockey League (NHL). Born and raised in Richmond Hill, Ontario, the Oilers selected him first overall in the 2015 NHL Entry Draft.McDavid is often regarded as one of the best players in the world, and his offensive dominance has drawn comparisons to ...\", name='duckduckgo_search', tool_call_id='call_nJf7KLPeF8x0XT0ACL2Ib3YC'), ToolMessage(content='September 12, 2023. There are not many honours in team sports bigger than being named captain. That honour was given to Winnipeg Jet forward Adam Lowry officially Tuesday morning as he becomes the ... - Sep 12, 2023 After a season without a captain, the Winnipeg Jets have named their new leader. Centre Adam Lowry, who is about to enter his 10th season as a Jet, was given the honour Tuesday. Posted September 12, 2023 9:29 am. Centre Adam Lowry was named the Winnipeg Jets new captain on Tuesday. Lowry is the third Jets captain since the team moved from Atlanta to Winnipeg in 2011. He follows Andrew Ladd and Blake Wheeler, who served as captain for five and six years respectively. The Winnipeg Jets will have a captain for the 2023-24 season. After going captain-less in 2022-23, the Winnipeg Jets unveiled Adam Lowry as the club\\'s new captain on Tuesday morning. \"When I ... Lowry is the third Jets captain since the team moved from Atlanta in 2011. After going without a captain for the 2022-23 season, Winnipeg chose the rugged centre over alternate captains Josh Morrissey and Mark Scheifele to succeed Blake Wheeler and Andrew Ladd.', name='duckduckgo_search', tool_call_id='call_XW5RcQV79qUvW8iQlHcacQ4r'), ToolMessage(content=\"Sanderson signed an eight-year deal worth $64.4 million prior to the 2023-24 season and he's undoubtedly one of the pillars of the franchise, even if captain Brady Tkachuk is the face of it. The 2023-24 Ottawa Senators season was the 32nd season of the Ottawa Senators of the National Hockey League (NHL). The Senators failed to return to the Stanley Cup playoffs for the seventh straight season after losing 4-3 to the New Jersey Devils on April 6, 2024 at the 76th game of the season. This was the Senators' first season under the new ownership of Michael Andlauer. Tkachuk's Captains; 5050; How You Can Help; ... ‚Äî Ottawa Senators (@Senators) October 23, 2023. ... The latter part of his tenure saw him playing alongside current leaders Brady Tkachuk and ... Back in his NHL playing days from 1992-2007, Ottawa Senators head coach Travis Green was a feisty, tenacious forward who played on the edge. So it should come as no surprise that Green is a fan of ... That should play well with some of Ottawa's young stars like forward Tim Stutzle, 22, whose 70 points last season (18 goals, 52 assists) were 20 fewer than the 90 (39 goals, 51 assists) he had ...\", name='duckduckgo_search', tool_call_id='call_OxSMVguUNBhUP5eCSfGawitd')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='Here are the current captains of the NHL teams in Canada for the 2023-24 season:\\n\\n1. **Toronto Maple Leafs**: Auston Matthews\\n2. **Montreal Canadiens**: Nick Suzuki\\n3. **Vancouver Canucks**: Quinn Hughes\\n4. **Calgary Flames**: Mikael Backlund\\n5. **Edmonton Oilers**: Connor McDavid\\n6. **Winnipeg Jets**: Adam Lowry\\n7. **Ottawa Senators**: Brady Tkachuk', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 3001, 'total_tokens': 3108}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-3742d783-9436-495e-9d41-595720c01410-0', usage_metadata={'input_tokens': 3001, 'output_tokens': 107, 'total_tokens': 3108})]\n",
            "\n",
            "\n",
            "\n",
            "Here are the current captains of the NHL teams in Canada for the 2023-24 season:\n",
            "\n",
            "1. **Toronto Maple Leafs**: Auston Matthews\n",
            "2. **Montreal Canadiens**: Nick Suzuki\n",
            "3. **Vancouver Canucks**: Quinn Hughes\n",
            "4. **Calgary Flames**: Mikael Backlund\n",
            "5. **Edmonton Oilers**: Connor McDavid\n",
            "6. **Winnipeg Jets**: Adam Lowry\n",
            "7. **Ottawa Senators**: Brady Tkachuk\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=user_query)]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "print(chunk['agent']['messages'][0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MY TRIAL #2: Are Any of the Captains Co-authors of a Paper on Arxiv?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Lld11qQATC5PKL4eUbBEw8zi', 'function': {'arguments': '{\"query\": \"current captains of NHL teams in Canada 2023\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_2ySl5aV58PA9G0iil5Yoetqw', 'function': {'arguments': '{\"query\": \"NHL captain\"}', 'name': 'arxiv'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 59, 'prompt_tokens': 181, 'total_tokens': 240}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d4b898f9-bc7e-420a-828f-7ed2b635868d-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'current captains of NHL teams in Canada 2023'}, 'id': 'call_Lld11qQATC5PKL4eUbBEw8zi', 'type': 'tool_call'}, {'name': 'arxiv', 'args': {'query': 'NHL captain'}, 'id': 'call_2ySl5aV58PA9G0iil5Yoetqw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 181, 'output_tokens': 59, 'total_tokens': 240})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content='Here\\'s the full list of captains (and teams without one) for the 2023-24 season. Brad Marchand was named captain of the Boston Bruins prior to the start of the 2023-24 NHL season. can vary by team, and there are currently five clubs that do not have a captain. After five teams anointed new players to wear the \"C\" this season, 27 of 32 clubs have a player with that designation. Here is the full list. ... Who is each NHL team\\'s captain? Current list for 2023-24. 35d ESPN staff. NHL trades we\\'d like to see before the deadline: Rangers, Golden Knights take big swings. Who is each NHL team\\'s captain? Current list for 2023-24. 34d ESPN staff. McDavid, then who? The NHL\\'s best centers right now, according to players and execs. 24/11/2023 18:18. Not all 32 teams currently have captains, but most do. Here is the current captain situation for every NHL squad.', name='duckduckgo_search', tool_call_id='call_Lld11qQATC5PKL4eUbBEw8zi'), ToolMessage(content=\"Published: 2012-01-09\\nTitle: Identifying the greatest team and captain - A complex network approach to cricket matches\\nAuthors: Satyam Mukherjee\\nSummary: We consider all Test matches played between 1877 and 2010 and One Day\\nInternational (ODI) matches played between 1971 and 2010. We form directed and\\nweighted networks of teams and also of their captains. The success of a team\\n(or captain) is determined by the 'quality' of wins and not on the number of\\nwins alone. We apply the diffusion based PageRank algorithm on the networks to\\naccess the importance of wins and rank the teams and captains respectively. Our\\nanalysis identifies {\\\\it Australia} as the best team in both forms of cricket\\n$-$ Test and ODI. {\\\\it Steve Waugh} is identified as the best captain in Test\\ncricket and {\\\\it Ricky Ponting} is the best captain in the ODI format. We also\\ncompare our ranking scheme with the existing ranking schemes which include the\\nReliance ICC Ranking. Our method does not depend on `external' criteria in\\nranking of teams (captains). The purpose of this paper is to introduce a\\nrevised ranking of cricket teams and to quantify the success of the captains.\\n\\nPublished: 2016-06-09\\nTitle: Individual Performance and Leader's Laterality in Interactive Contests\\nAuthors: Satyam Mukherjee\\nSummary: Left-handedness is known to provide an intrinsic and tactical advantage at\\ntop level in many sports involving interactive contests. Again, most of the\\nrenowned leaders of the world are known to have been left-handed. Leadership\\nplays an important role in politics, sports and mentorship. In this paper we\\nshow that Cricket captains who bat left-handed have a strategic advantage over\\nthe right-handed captains in One Day International (ODI) and Test matches. The\\npresent study involving 46 left-handed captains and 148 right-handed captains\\nin ODI matches, reveal a strong relation between leader's laterality and\\nteam-member performance, demonstrating the critical importance of\\nleft-handedness and successful leadership. The odds for superior batting\\nperformance in an ODI match under left-handed captains are 89% higher than the\\nodds under right-handed captains. Our study shows that left-handed captains are\\nmore successful in extracting superior performance from the batsmen and bowlers\\nin ODI and Test matches; perhaps indicating left-handed leaders are better\\nmotivators as leaders when compared to right-handed captains.\\n\\nPublished: 2011-09-25\\nTitle: The Never Ending Gale: its Role in Captain Robert F. Scott and his Companions' Deaths\\nAuthors: Krzysztof Sienicki\\nSummary: Polar historians and enthusiasts are aware that toward the end of March 1912,\\nCaptain Robert F. Scott reported in his journal a meteorological event, which\\nwas extraordinary as far as its length and strength was concerned. This event\\nwas the gale which according to Captain Scott, lasted nine/ten days. Were the\\nlaws of physics suspended at the end of March 1912 in the Antarctic? I have\\nshown that the near surface winds in the Antarctic are self-organized\\ncritically and that the winds over the continent form an ergodic system. I have\\npresented an analysis of wind events in the proximity of Captain Scott's camp\\nand at Ross Island. By comparing wind events at these locations, and performing\\nan analysis of a gale's wind duration and strength at One Ton Dep\\\\^ot, I\\nconcluded that Captain Scott's wind record was highly inaccurate. I concluded\\nthat the nine/ten day gale described by Captain Scott, that lasted from March\\n21 to 29, did not take place. This result combined with my previous analysis of\\nCaptain Scott's temperature record, shows that two black swan meteorological\\nevents: February 27-March 19, 1912 - Extreme Cold Snap and March 21-29, 1912 -\\nNever Ending Gale reported by Captain Scott, did not take place. Therefore, I\\nconclude that the deaths of Scott, Wilson and Bowers were a matter of choice\\nrather than chance.\", name='arxiv', tool_call_id='call_2ySl5aV58PA9G0iil5Yoetqw')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"### Current Captains of NHL Teams in Canada (2023-24 Season)\\n\\n1. **Calgary Flames**: Mikael Backlund\\n2. **Edmonton Oilers**: Connor McDavid\\n3. **Montreal Canadiens**: Nick Suzuki\\n4. **Ottawa Senators**: Brady Tkachuk\\n5. **Toronto Maple Leafs**: John Tavares\\n6. **Vancouver Canucks**: Quinn Hughes\\n7. **Winnipeg Jets**: Adam Lowry\\n\\n### Papers on Arxiv Mentioning Captains\\n\\n1. **Title**: Identifying the greatest team and captain - A complex network approach to cricket matches\\n   - **Authors**: Satyam Mukherjee\\n   - **Summary**: This paper analyzes cricket matches to rank teams and captains using a network approach. It identifies Australia as the best team and Steve Waugh and Ricky Ponting as the best captains in Test and ODI formats, respectively.\\n\\n2. **Title**: Individual Performance and Leader's Laterality in Interactive Contests\\n   - **Authors**: Satyam Mukherjee\\n   - **Summary**: This study shows that left-handed cricket captains have a strategic advantage over right-handed captains in ODI and Test matches, demonstrating the importance of left-handedness in leadership.\\n\\n3. **Title**: The Never Ending Gale: its Role in Captain Robert F. Scott and his Companions' Deaths\\n   - **Authors**: Krzysztof Sienicki\\n   - **Summary**: This paper analyzes meteorological events reported by Captain Robert F. Scott during his Antarctic expedition, concluding that the reported extreme weather events did not occur and suggesting that the deaths of Scott and his companions were a matter of choice rather than chance.\\n\\nWhile these papers focus on cricket and historical events, they provide insights into the roles and impacts of captains in their respective fields.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 391, 'prompt_tokens': 1362, 'total_tokens': 1753}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-6e5ffd3a-aa80-4bd9-b767-6efd761d71ae-0', usage_metadata={'input_tokens': 1362, 'output_tokens': 391, 'total_tokens': 1753})]\n",
            "\n",
            "\n",
            "\n",
            "### Current Captains of NHL Teams in Canada (2023-24 Season)\n",
            "\n",
            "1. **Calgary Flames**: Mikael Backlund\n",
            "2. **Edmonton Oilers**: Connor McDavid\n",
            "3. **Montreal Canadiens**: Nick Suzuki\n",
            "4. **Ottawa Senators**: Brady Tkachuk\n",
            "5. **Toronto Maple Leafs**: John Tavares\n",
            "6. **Vancouver Canucks**: Quinn Hughes\n",
            "7. **Winnipeg Jets**: Adam Lowry\n",
            "\n",
            "### Papers on Arxiv Mentioning Captains\n",
            "\n",
            "1. **Title**: Identifying the greatest team and captain - A complex network approach to cricket matches\n",
            "   - **Authors**: Satyam Mukherjee\n",
            "   - **Summary**: This paper analyzes cricket matches to rank teams and captains using a network approach. It identifies Australia as the best team and Steve Waugh and Ricky Ponting as the best captains in Test and ODI formats, respectively.\n",
            "\n",
            "2. **Title**: Individual Performance and Leader's Laterality in Interactive Contests\n",
            "   - **Authors**: Satyam Mukherjee\n",
            "   - **Summary**: This study shows that left-handed cricket captains have a strategic advantage over right-handed captains in ODI and Test matches, demonstrating the importance of left-handedness in leadership.\n",
            "\n",
            "3. **Title**: The Never Ending Gale: its Role in Captain Robert F. Scott and his Companions' Deaths\n",
            "   - **Authors**: Krzysztof Sienicki\n",
            "   - **Summary**: This paper analyzes meteorological events reported by Captain Robert F. Scott during his Antarctic expedition, concluding that the reported extreme weather events did not occur and suggesting that the deaths of Scott and his companions were a matter of choice rather than chance.\n",
            "\n",
            "While these papers focus on cricket and historical events, they provide insights into the roles and impacts of captains in their respective fields.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Who is the current captain of the team that plays ice hockey in the NHL in Canada?And, is there any paper on Arxiv that mentions one of the captains?\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "print(chunk['agent']['messages'][0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"tool_calls\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"tool_calls\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "026a3aa3-1c3e-4016-b0a3-fe98b1d25399"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_SlqUFzNvQx8HrcewiEdtHXdI', 'function': {'arguments': '{\"query\": \"QLoRA\"}', 'name': 'arxiv'}, 'type': 'function'}, {'id': 'call_iMyxkqyfRsSPDjh60EjvYd1N', 'function': {'arguments': '{\"query\": \"latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 50, 'prompt_tokens': 173, 'total_tokens': 223}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-5acff216-b8b3-4149-9e40-1dba734858a1-0', tool_calls=[{'name': 'arxiv', 'args': {'query': 'QLoRA'}, 'id': 'call_SlqUFzNvQx8HrcewiEdtHXdI', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'latest Tweet'}, 'id': 'call_iMyxkqyfRsSPDjh60EjvYd1N', 'type': 'tool_call'}], usage_metadata={'input_tokens': 173, 'output_tokens': 50, 'total_tokens': 223})]\n",
            "[{'args': {'query': 'QLoRA'},\n",
            "  'id': 'call_SlqUFzNvQx8HrcewiEdtHXdI',\n",
            "  'name': 'arxiv',\n",
            "  'type': 'tool_call'},\n",
            " {'args': {'query': 'latest Tweet'},\n",
            "  'id': 'call_iMyxkqyfRsSPDjh60EjvYd1N',\n",
            "  'name': 'duckduckgo_search',\n",
            "  'type': 'tool_call'}]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: arxiv\n",
            "[ToolMessage(content='Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2024-05-27\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnologies derived from the perspective of unified information: (1)\\nstatistics-based Information Calibration Quantization allows the quantized\\nparameters of LLM to retain original information accurately; (2)\\nfinetuning-based Information Elastic Connection makes LoRA utilizes elastic\\nrepresentation transformation with diverse information. Comprehensive\\nexperiments show that IR-QLoRA can significantly improve accuracy across LLaMA\\nand LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\\nimprovement on MMLU compared with the state-of-the-art methods. The significant\\nperformance gain requires only a tiny 0.31% additional time consumption,\\nrevealing the satisfactory efficiency of our IR-QLoRA. We highlight that\\nIR-QLoRA enjoys excellent versatility, compatible with various frameworks\\n(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\\nThe code is available at https://github.com/htqin/ir-qlora.\\n\\nPublished: 2024-06-12\\nTitle: Exploring Fact Memorization and Style Imitation in LLMs Using QLoRA: An Experimental Study and Quality Assessment Methods\\nAuthors: Eugene Vyborov, Oleksiy Osypenko, Serge Sotnyk\\nSummary: There are various methods for adapting LLMs to different domains. The most\\ncommon methods are prompting, finetuning, and RAG. In this w', name='arxiv', tool_call_id='call_SlqUFzNvQx8HrcewiEdtHXdI'),\n",
            " ToolMessage(content=\"Twitter. The latest Twitter news and updates. Twitter is a social networking service, primarily microblogging but also a picture and video sharing service, founded by Jack Dorsey, Noah Glass, Biz ... Twitter has added a warning to one of President Donald Trump's tweets about protests in Minneapolis. The company says the tweet violated the platform's rules about glorifying violence. U.S ... Sept. 15, 2023. The federal prosecutors who charged former President Donald J. Trump with a criminal conspiracy over his attempts to overturn the 2020 election obtained 32 private messages from ... Twitter Blue's Chaotic Revamp: The overhaul of the subscription service that lets users buy verified badges was the first big test for Musk as the platform's new owner. It didn't go well . Prosecutors from the special counsel's office, for example, included tweets in which Trump pressured Vice President Mike Pence to reject Electoral College votes before Congress and tweets urging ...\", name='duckduckgo_search', tool_call_id='call_iMyxkqyfRsSPDjh60EjvYd1N')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9MaXFoJ7uMALq27vg4r2TFd1', 'function': {'arguments': '{\"query\": \"Tim Dettmers latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_uCzNcAfRfczFX6MM4xJRGfOk', 'function': {'arguments': '{\"query\": \"Artidoro Pagnoni latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_K7q3scFOP36ho3PYpkS6seLO', 'function': {'arguments': '{\"query\": \"Ari Holtzman latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_p1dHVmSvNGyRVXBqWQeZx6AQ', 'function': {'arguments': '{\"query\": \"Luke Zettlemoyer latest Tweet\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 1390, 'total_tokens': 1498}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-996c0f11-c375-43c2-a473-77b9557a0f32-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers latest Tweet'}, 'id': 'call_9MaXFoJ7uMALq27vg4r2TFd1', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Artidoro Pagnoni latest Tweet'}, 'id': 'call_uCzNcAfRfczFX6MM4xJRGfOk', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Ari Holtzman latest Tweet'}, 'id': 'call_K7q3scFOP36ho3PYpkS6seLO', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Luke Zettlemoyer latest Tweet'}, 'id': 'call_p1dHVmSvNGyRVXBqWQeZx6AQ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1390, 'output_tokens': 108, 'total_tokens': 1498})]\n",
            "[{'args': {'query': 'Tim Dettmers latest Tweet'},\n",
            "  'id': 'call_9MaXFoJ7uMALq27vg4r2TFd1',\n",
            "  'name': 'duckduckgo_search',\n",
            "  'type': 'tool_call'},\n",
            " {'args': {'query': 'Artidoro Pagnoni latest Tweet'},\n",
            "  'id': 'call_uCzNcAfRfczFX6MM4xJRGfOk',\n",
            "  'name': 'duckduckgo_search',\n",
            "  'type': 'tool_call'},\n",
            " {'args': {'query': 'Ari Holtzman latest Tweet'},\n",
            "  'id': 'call_K7q3scFOP36ho3PYpkS6seLO',\n",
            "  'name': 'duckduckgo_search',\n",
            "  'type': 'tool_call'},\n",
            " {'args': {'query': 'Luke Zettlemoyer latest Tweet'},\n",
            "  'id': 'call_p1dHVmSvNGyRVXBqWQeZx6AQ',\n",
            "  'name': 'duckduckgo_search',\n",
            "  'type': 'tool_call'}]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "Tool Used: duckduckgo_search\n",
            "[ToolMessage(content='‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Ann Coulter is facing the wrath of social media after posting ‚Äî then deleting ‚Äî a tweet about Tim Walz\\'s son, Gus. Following the third night of the 2024 Democratic National Convention, the ... Former national security adviser H.R. McMaster said on Sunday he \"would never disparage\" someone\\'s military service, when asked about the controversy surrounding the Democratic vice ... Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. Stories and news about Tim Dettmers from GeekWire.', name='duckduckgo_search', tool_call_id='call_9MaXFoJ7uMALq27vg4r2TFd1'),\n",
            " ToolMessage(content=\"Artidoro Pagnoni. artidoro. Follow. mrm8488's profile picture Weyaxi's profile picture nezubn's profile picture. ... artidoro/model-tvergho. Updated Nov 18, 2023. artidoro/model-vinaic. Updated Nov 18, 2023. artidoro/model-vinaia. Updated Nov 18, 2023. datasets. None public yet. Company We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly ... efficient finetuning of quantized LLMs. AUTHORs: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer Authors Info & Claims. NIPS '23: Proceedings of the 37th International Conference on Neural Information Processing Systems. Article No.: 441, Pages 10088 - 10115. Published: 30 May 2024 Publication History. Pagnoni et al. (2021) Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics . In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , pages ... Fine-tuning large language models (LLMs) is a common practice to adapt them for specific tasks, but it can be computationally expensive. LoRA (Low-Rank Adaptation) is a technique that makes this process more efficient by introducing small adapter modules to the model. These adapters capture task-specific knowledge without modifying the original ...\", name='duckduckgo_search', tool_call_id='call_uCzNcAfRfczFX6MM4xJRGfOk'),\n",
            " ToolMessage(content='The heated on-air dispute last night between MSNBC\\'s Ari Melber and Donald Trump campaign adviser Corey Lewandowski didn\\'t end with Wednesday\\'s segment: Today, Lewandowski tweeted a video in ... Ari Melber warns Trump campaign advisor Corey Lewandowski he \\'will be potentially in a defamation situation\\' on Wednesday\\'s episode of The Beat (MSNBC) \"I did not say that. That is a false ... MSNBC host Ari Melber, during an interview with Trump campaign adviser Corey Lewandowski on Wednesday, threatened him with a defamation lawsuit for quoting the anchor calling the former President ... In this blog series, we are introducing our research papers at the ICASSP 2024 and here is a list of them. #1. MELS-TTS: Multi-Emotion Multi-Lingual Multi-Speaker Text-To-Speech System via Disentangled Style Tokens (Samsung Research) #2. Latent Filling: Latent Space Data Augmentation for Zero-Shot Speech Synthesis (Samsung Research) #3. Aug. 31. 46% Trump. 49% Harris. Ruth Igielnik Staff editor, polling. As August winds down and the heat of campaign season begins, Kamala Harris and Donald J. Trump are neck-and-neck in our ...', name='duckduckgo_search', tool_call_id='call_K7q3scFOP36ho3PYpkS6seLO'),\n",
            " ToolMessage(content=\"Luke Zettlemoyer is a research manager and site lead for FAIR Seattle. He is also a Professor in the Allen School of Computer Science & Engineering at the University of Washington. His research is in empirical computational semantics, where the goal is to build models that recover representations of the meaning of natural language text. Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open source and open science. We explore the grounding problem, the need for visual grounding and embodiment in \\ufeff Twitter \\ufeff Reddit. Join our list for notifications and early access to events ... About this Episode. Today we're joined by Luke Zettlemoyer, professor at University of Washington and a research manager at Meta. In our conversation with Luke, we cover multimodal generative AI, the effect of data on models, and the significance of open ... Evaluating the factuality of long-form text generated by large language models (LMs) is nontrivial because (1) generations often contain a mixture of... Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard ...\", name='duckduckgo_search', tool_call_id='call_p1dHVmSvNGyRVXBqWQeZx6AQ')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"Here are the latest updates and information about the authors of the QLoRA paper:\\n\\n### Tim Dettmers\\n- **Latest Tweet**: Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization. He recently accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required.\\n\\n### Artidoro Pagnoni\\n- **Latest Tweet**: Artidoro Pagnoni's latest updates include contributions to various models such as model-tvergho, model-vinaic, and model-vinaia. He is actively involved in the efficient finetuning of quantized LLMs and has co-authored the QLoRA paper.\\n\\n### Ari Holtzman\\n- **Latest Tweet**: Ari Holtzman has been involved in discussions and research presentations, including a blog series introducing research papers at ICASSP 2024. He is also known for his work on multi-emotion, multi-lingual, and multi-speaker text-to-speech systems.\\n\\n### Luke Zettlemoyer\\n- **Latest Tweet**: Luke Zettlemoyer is a research manager and site lead for FAIR Seattle, as well as a Professor at the University of Washington. He has been involved in discussions about multimodal generative AI, the effect of data on models, and the significance of open source and open science. He recently participated in a conversation covering these topics.\\n\\nIf you need more detailed information or specific tweets, please let me know!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 2714, 'total_tokens': 3050}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-1080b875-24ef-45cf-a555-eff662f8f1b4-0', usage_metadata={'input_tokens': 2714, 'output_tokens': 336, 'total_tokens': 3050})]\n",
            "\n",
            "\n",
            "\n",
            "Here are the latest updates and information about the authors of the QLoRA paper:\n",
            "\n",
            "### Tim Dettmers\n",
            "- **Latest Tweet**: Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization. He recently accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required.\n",
            "\n",
            "### Artidoro Pagnoni\n",
            "- **Latest Tweet**: Artidoro Pagnoni's latest updates include contributions to various models such as model-tvergho, model-vinaic, and model-vinaia. He is actively involved in the efficient finetuning of quantized LLMs and has co-authored the QLoRA paper.\n",
            "\n",
            "### Ari Holtzman\n",
            "- **Latest Tweet**: Ari Holtzman has been involved in discussions and research presentations, including a blog series introducing research papers at ICASSP 2024. He is also known for his work on multi-emotion, multi-lingual, and multi-speaker text-to-speech systems.\n",
            "\n",
            "### Luke Zettlemoyer\n",
            "- **Latest Tweet**: Luke Zettlemoyer is a research manager and site lead for FAIR Seattle, as well as a Professor at the University of Washington. He has been involved in discussions about multimodal generative AI, the effect of data on models, and the significance of open source and open science. He recently participated in a conversation covering these topics.\n",
            "\n",
            "If you need more detailed information or specific tweets, please let me know!\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Search Arxiv for the QLoRA paper, then search each of the authors to find out their latest Tweet using DuckDuckGo.\")]}\n",
        "\n",
        "async for chunk in compiled_graph.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        if node == \"action\":\n",
        "          print(f\"Tool Used: {values['messages'][0].name}\")\n",
        "        pprint.pprint(values[\"messages\"])\n",
        "        if node == \"agent\":\n",
        "           if values[\"messages\"][0].tool_calls:\n",
        "              pprint.pprint(values[\"messages\"][0].tool_calls)\n",
        "        print(\"\\n\\n\")\n",
        "  \n",
        "print(chunk['agent']['messages'][0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ACTIVITY #2:\n",
        "\n",
        "1.  STEP 1\n",
        "\n",
        "-   Agent code appends to the State with an AIMessage containing kwarg tool_calls to call the Arxiv tool using the keyword QLoRa.\n",
        "Contrary to my expectation, however, I also found that there were tool calls to DuckDuckGo in step 1 - please see the output of the previous cell!\n",
        "\n",
        "2.  STEP 2\n",
        "\n",
        "-   The calls to the two tools are sent to the Tool Node ie the Action node where they are executed and the ToolMessages are appended to the sequence of messages.  The action node then passes the full set of messages back to the Agent node.\n",
        "\n",
        "3.  STEP 3\n",
        "\n",
        "-   The Agent node receives the full set of messages (latest are the ToolMessages).  This time the agent node formulates very specific search queries, looking for the latest tweet by each of the authors of the paper.\n",
        "\n",
        "4.  STEP 4\n",
        "\n",
        "-   The action node executes the searches and appends four more ToolMessages to the sequence of messages.  The full set is again passed back to the Agent node.\n",
        "\n",
        "5.  STEP 5\n",
        "\n",
        "-   The Agent node compiles a response to the user, as it has all the information needed.  No Tool Call is made in this step, so the cycle terminates at this point with an AIMessage written to the sequence of messages (this is the response to the user question)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | compiled_graph | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b160f3e4-89d1-4411-ed96-fe17b3cad200"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here's a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus or database. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input to the generative model. This means that the generative model has access to additional context or information that can help it produce more accurate and relevant responses.\\n\\n3. **Generation**: Finally, the generative model, such as GPT-3 or BERT, uses the augmented input to generate a response. The additional context provided by the retrieved documents helps the model generate more informed and contextually appropriate responses.\\n\\nRAG is particularly useful in scenarios where the generative model alone might not have enough information to produce a high-quality response, such as in open-domain question answering, dialogue systems, and other applications requiring access to a large knowledge base.\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    ### YOUR CODE HERE\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    ### YOUR CODE HERE\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "> NOTE: Feel free to indicate if this is problematic or not"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANSWER #3:\n",
        "\n",
        "The correct answers are associated with the questions based on their corresponding positions in the `questions` list and the `answers` list.  This is problematic for many reasons.  \n",
        "\n",
        "First, there is a chance that the orders in the questions and answers lists may change if someone accidentally introduces new question/answer pairs and does not put them in the proper order.  \n",
        "\n",
        "Second, as a more general principle, there is nothing in the setup that shows this connection between questions and answers!  Far better to set up the data in a more structured manner as a separate effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ANSWER #4:\n",
        "\n",
        "The metric is very strict!  It requires that the RAG pipeline response must contain the EXACT strings provided in the `must_mention` lists for each query.  This includes matching on case as well!  \n",
        "\n",
        "1.  The pipeline should be given credit for output that is semantically close to the right answer even if the words/phrases are different.  Some version of cosine similarity or other closeness metric can be used.  \n",
        "\n",
        "2.  Also, in the case of questions like author of the paper, the pipeline should be given credit for naming one of the four authors, not necessarily only the lead author.  So, perhaps provide a list of acceptable authors, and give credit if one of them is in the response.  The same idea applies to other questions as well, where the acceptable response might be one of several words in a list.  In other words, don't be extremely stringent - partial credit may be needed!!\n",
        "\n",
        "3.  Finally, give credit even if the answer does not match with the right `case` e.g., lower case vs camelcase, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "8b98fff1-bd75-4dbe-cadc-a76d8a82577c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 6a240b44' at:\n",
            "https://smith.langchain.com/o/d6cd6bbe-53a2-5b54-80e6-5a211fe37a3d/datasets/a84bce53-4816-4e81-ad2b-fb1bda58b72d/compare?selectedSessions=de5f105c-a352-4f1f-add0-d0e8177d1a63\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - b51d6bd7 at:\n",
            "https://smith.langchain.com/o/d6cd6bbe-53a2-5b54-80e6-5a211fe37a3d/datasets/a84bce53-4816-4e81-ad2b-fb1bda58b72d\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>a86e6c8e-6d4b-4dce-8d0a-1b8c99055d45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.681292</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.811985</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.503537</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.138632</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.437000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.597860</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>8.019555</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                    False   NaN             NaN   \n",
              "freq                       4   NaN             NaN   \n",
              "mean                     NaN   NaN        4.681292   \n",
              "std                      NaN   NaN        1.811985   \n",
              "min                      NaN   NaN        2.503537   \n",
              "25%                      NaN   NaN        4.138632   \n",
              "50%                      NaN   NaN        4.437000   \n",
              "75%                      NaN   NaN        4.597860   \n",
              "max                      NaN   NaN        8.019555   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     a86e6c8e-6d4b-4dce-8d0a-1b8c99055d45  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 6a240b44',\n",
              " 'results': {'4870dc51-3a1c-42df-a67a-3628d86399e5': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('685c8d42-a9c3-49cb-8798-9fb2cc33657f'), target_run_id=None)],\n",
              "   'execution_time': 4.649164,\n",
              "   'run_id': 'a86e6c8e-6d4b-4dce-8d0a-1b8c99055d45',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) uses a combination of optimizers to manage memory efficiently during the fine-tuning of large language models. Specifically, it introduces \"paged optimizers\" to handle memory spikes. This approach allows QLoRA to backpropagate gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA), significantly reducing memory usage without sacrificing performance.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '4ac32879-0d11-405a-823d-1efd4acfbe07': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('4d3cde2b-5d2e-46c6-8331-4a580c115596'), target_run_id=None)],\n",
              "   'execution_time': 4.041492,\n",
              "   'run_id': '2a18bde8-58ea-45a2-adb7-6c811b62f185',\n",
              "   'output': 'In the QLoRA paper, a new data type called **4-bit NormalFloat (NF4)** was introduced. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of quantized large language models (LLMs).',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  'ebec19eb-d063-48db-8024-0bee0f9ae525': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('5250e1dc-6b31-4243-8c4d-9574fcc51cae'), target_run_id=None)],\n",
              "   'execution_time': 4.44395,\n",
              "   'run_id': '7dd82f7e-c4bc-40ef-9bf0-c97c8af1a67f',\n",
              "   'output': \"A Retrieval Augmented Generation (RAG) system is a type of artificial intelligence model that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated responses. Here's a breakdown of the components and how they work together:\\n\\n1. **Retrieval Component**:\\n   - This part of the system retrieves relevant documents or pieces of information from a large corpus or database based on the input query.\\n   - It uses techniques such as keyword matching, semantic search, or other information retrieval methods to find the most relevant content.\\n\\n2. **Generation Component**:\\n   - This part of the system generates a response based on the retrieved information and the input query.\\n   - It typically uses a language model (e.g., GPT-3, BERT) to generate coherent and contextually appropriate text.\\n\\n3. **Integration**:\\n   - The retrieved documents or information are used to inform and guide the generation process.\\n   - The generation model can use the retrieved content to produce more accurate, informative, and contextually relevant responses.\\n\\n### Benefits of RAG Systems:\\n- **Improved Accuracy**: By leveraging relevant information from a large corpus, the system can provide more accurate and factually correct responses.\\n- **Contextual Relevance**: The retrieval component ensures that the generated responses are contextually relevant to the input query.\\n- **Scalability**: RAG systems can handle a wide range of topics and queries by accessing a vast amount of information.\\n\\n### Applications:\\n- **Question Answering**: Providing detailed and accurate answers to user queries.\\n- **Customer Support**: Assisting in customer service by retrieving and generating relevant responses to customer inquiries.\\n- **Content Creation**: Aiding in the creation of articles, reports, and other content by retrieving relevant information and generating coherent text.\\n\\nOverall, RAG systems represent a powerful approach to combining the strengths of both retrieval-based and generation-based methods to enhance the capabilities of AI-driven text generation.\",\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '4015dd5d-f7b9-4526-90c2-1380798fb05e': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('fcf0184a-c4cd-4c37-ab3c-e18da2aa8c9b'), target_run_id=None)],\n",
              "   'execution_time': 4.430051,\n",
              "   'run_id': '168b06bc-dbc4-438f-9059-bcda5fe81bbc',\n",
              "   'output': 'The QLoRA paper titled \"Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\" was authored by:\\n\\n- Haotong Qin\\n- Xudong Ma\\n- Xingyu Zheng\\n- Xiaoyang Li\\n- Yang Zhang\\n- Shouda Liu\\n- Jie Luo\\n- Xianglong Liu\\n- Michele Magno\\n\\nYou can find more details and the paper itself [here](https://github.com/htqin/ir-qlora).',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  '55d1f245-70b8-4423-b389-36b5f666f206': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0454eaed-9bc9-4306-9ccd-12a346ae5969'), target_run_id=None)],\n",
              "   'execution_time': 2.503537,\n",
              "   'run_id': 'f5cec981-bfdc-4d5c-b109-242f5d0a3bc8',\n",
              "   'output': 'As of 2023, TensorFlow is considered one of the most popular deep learning frameworks. It is an open-source library supported by Google and is widely used for numerical computation and deep learning tasks. Major corporations like Airbnb and Intel utilize TensorFlow for their projects.',\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  '08053d1c-bdff-47d6-9e25-a1d9286d3d23': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bbdb82b3-8338-42db-8701-973ae34e54ab'), target_run_id=None)],\n",
              "   'execution_time': 8.019555,\n",
              "   'run_id': '85b6ce33-170b-43d9-889b-1ec02854aefb',\n",
              "   'output': 'The LoRA (Low-Rank Adaptation) system has made several significant improvements, particularly in the context of federated learning and IoT applications. Here are some key advancements:\\n\\n1. **Communication Efficiency**:\\n   - **Federated LoRA with Sparse Communication (FLASC)**: This method applies sparsity to LoRA during communication, allowing clients to locally fine-tune the entire LoRA module. It achieves up to 10 times less communication while maintaining performance comparable to dense LoRA. This is particularly beneficial in communication-constrained settings like cross-device federated learning.\\n\\n2. **System-Specific Constraints**:\\n   - The FLASC approach highlights the importance of considering system-specific constraints when developing communication-efficient finetuning approaches. It also offers benefits in terms of heterogeneity and privacy, making it a competitive baseline for future work in federated finetuning.\\n\\n3. **IoT Applications**:\\n   - **Open LoRa for IoT**: This design and implementation allow users to build private LoRa networks for various IoT applications. The improved software architecture of the LoRa network server, which is open-source, ensures scalability and flexibility. This is achieved by dividing the server into four decoupled modules and using a messaging system based on streaming data for interaction between modules.\\n\\n4. **Energy Efficiency and Cost-Effectiveness**:\\n   - LoRa networks are designed to be low-cost and energy-efficient, making them suitable for large-scale deployments in IoT applications like smart cities and precision agriculture.\\n\\n5. **Autonomous LPWAN Networks**:\\n   - LoRa networking allows for the creation of autonomous Low Power Wide Area Networks (LPWAN) without relying on third-party infrastructure. This autonomy is crucial for applications that require reliable and secure communication over large areas.\\n\\nThese improvements make LoRA a versatile and efficient solution for both federated learning and IoT applications, addressing key challenges such as communication efficiency, scalability, flexibility, and energy efficiency.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add to our existing conditional edge to obtain the behaviour we desire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "### Initialize Graph Object and Set Up Two Nodes\n",
        "\n",
        "In this step, we initialize a new graph object `graph_with_helpfulness_check`.\n",
        "\n",
        "We also set up two nodes: `agent` and `action` nodes and the associated functions that will be executed by these nodes.  So, e.g., `agent` will execute the `call_model` function and `action` will run the `tool_node` that provides access to the tools needed.\n",
        "\n",
        "So far, this is identical to the previous graph we set up earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", tool_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "### Set entry point to graph\n",
        "\n",
        "As before, we set the entry point to the graph as the `agent` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsXeF6xlaXOZ"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "### Check if the agent should terminate or continue\n",
        "\n",
        "This cell defines the function that determines if the agent should terminate or continue.\n",
        "\n",
        "1.  If the last message has a non-empty `tool_calls` kwarg, then the agent is routed to the `action` node.\n",
        "\n",
        "2.  If not, then:\n",
        "-   The `agent` will terminate if there are more than 10 Message objects in the AgentState.\n",
        "-   OR, if the `helpfulness_response` has returned a \"Y\" ie that the LLM call in this step determines that the final response is helpful as a response to the initial query.\n",
        "\n",
        "3.  If the `agent` has not accumulated more than 10 message objects in the AgentState, and if the response is not deemed to be helpful, the agent will continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "z_Sq3A9SaV1O"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def tool_call_or_helpful(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if last_message.tool_calls:\n",
        "    return \"action\"\n",
        "\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    return \"end\"\n",
        "  else:\n",
        "    return \"continue\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `tool_call_or_helpful` function!\n",
        "\n",
        "1.  Get the last ie most recent message from the state.  If this message has a non-empty tool-call, then execute the tool call.\n",
        "\n",
        "2.  If the last message is not a tool-call, then check if either of the following conditions is met:\n",
        "-   has the length of the list of messages in the agent state exceeded 10?  OR\n",
        "-   does the LLM call in this step deem the latest response to be helpful given the initial query?\n",
        "If the answer to either one of these questions is a 'yes', then the agent will terminate.\n",
        "\n",
        "3.  If neither condition in 2. is met, then the agent will continue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "### Add conditional Edges\n",
        "\n",
        "This step sets up conditional edges from the `agent` node to one of three alternatives:\n",
        "1.  If tool_call_or_helpful function returns `action`, then loop to `action` node\n",
        "2.  If tool_call_or_helpful function returns `end`, then terminate the agent.\n",
        "3.  If tool_call_or_helpful function returns `continue`, then loop back to `agent` node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tool_call_or_helpful,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"action\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "### Add Edge from Action to Agent\n",
        "\n",
        "This step adds an edge from action back to agent.  So, this means that every time the action node is executed, the agent will return to the agent node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "## Compile Graph\n",
        "\n",
        "This step compiles the graph.  After this step is completed, the agent is ready to execute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "### Run the Agent!\n",
        "\n",
        "1.  Run the agent in async streaming mode.\n",
        "2.  Print out the results as the agent is executing\n",
        "\n",
        "SEE RESULTS BELOW!!\n",
        "\n",
        "1.  On the first call, there are three search terms extracted and each of them is set up with a tool call to DuckDuckGoSearch.\n",
        "2.  The tool calls return relevant content for each question.\n",
        "3.  The final AI message has responses to all three questions posed in the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "22470df4-9aa7-4751-95b6-d30ce71b17db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_zd6hh34mqdpKx5k4T6iQlJXY', 'function': {'arguments': '{\"query\": \"LoRA machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_P1LNZqm0wydBVj0yeCopx4RK', 'function': {'arguments': '{\"query\": \"Tim Dettmers\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}, {'id': 'call_1qtQMLUZLSeDBWOmFhM4xEvE', 'function': {'arguments': '{\"query\": \"Attention in machine learning\"}', 'name': 'duckduckgo_search'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 171, 'total_tokens': 247}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-427d3690-d09b-4118-beb2-9aeb542ef1f2-0', tool_calls=[{'name': 'duckduckgo_search', 'args': {'query': 'LoRA machine learning'}, 'id': 'call_zd6hh34mqdpKx5k4T6iQlJXY', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Tim Dettmers'}, 'id': 'call_P1LNZqm0wydBVj0yeCopx4RK', 'type': 'tool_call'}, {'name': 'duckduckgo_search', 'args': {'query': 'Attention in machine learning'}, 'id': 'call_1qtQMLUZLSeDBWOmFhM4xEvE', 'type': 'tool_call'}], usage_metadata={'input_tokens': 171, 'output_tokens': 76, 'total_tokens': 247})]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'action'\n",
            "[ToolMessage(content=\"Let's jump on LoRA. Low-Rank Adaptation of LLMs (LoRA) So, in usual fine-tuning, we. Take a pretrained model. Do Transfer Learning over new training data to slightly adjust these pre-trained weights LoRA's approach to decomposing ( Œî W ) into a product of lower rank matrices effectively balances the need to adapt large pre-trained models to new tasks while maintaining computational efficiency. The intrinsic rank concept is key to this balance, ensuring that the essence of the model's learning capability is preserved with significantly ... Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning ($\\\\\\\\approx$100K prompt-response ... Feb 18, 2024. Share. Ever since the introduction of BERT in 2019, fine-tuning has been the standard approach to adapt large language models (LLMs) to downstream tasks. This changed with the introduction of LoRA (Hu et al 2021) which showed for the first time that the weight update matrix during fine-tuning can be drastically simplified using ... Feb 18, 2024. Low-rank adaptation (LoRA) is a machine learning technique that modifies a pretrained model (for example, an LLM or vision transformer) to better suit a specific, often smaller, dataset by adjusting only a small, low-rank subset of the model's parameters. This approach is important because it allows for efficient finetuning of ...\", name='duckduckgo_search', tool_call_id='call_zd6hh34mqdpKx5k4T6iQlJXY'), ToolMessage(content='Allen School Ph.D. student Tim Dettmers accepted the grand prize for QLoRA, a novel approach to finetuning pretrained models that significantly reduces the amount of GPU memory required ‚Äî from over 780GB to less than 48GB ‚Äî to finetune a 65B parameter model. With QLoRA, the largest publicly available models can be finetuned on a single ... ‚Äî Tim Dettmers is joining Ai2 as an AI researcher. Dettmers specializes in efficient deep learning at the intersection of machine learning, NLP, and computer systems with a focus on quantization ... Its purpose is to make cutting-edge research by Tim Dettmers, a leading academic expert on quantization and the use of deep learning hardware accelerators, accessible to the general public. QLoRA: One of the core contributions of bitsandbytes towards the democratization of AI. If you have a curiosity about how fancy graphics cards actually work, and why they are so well-suited to AI-type applications, then take a few minutes to read [Tim Dettmers] explain why this is so.‚Ä¶ Tim Dettmers. Video. Tech Moves: AI researcher Yejin Choi leaves Univ. of Washington and Allen Institute for AI. by Todd Bishop & Taylor Soper on August 2, 2024 August 2, 2024 at 11:59 am.', name='duckduckgo_search', tool_call_id='call_P1LNZqm0wydBVj0yeCopx4RK'), ToolMessage(content='Learn how attention mechanisms in deep learning enable models to focus on relevant information and improve performance in tasks such as machine translation, image captioning, and speech recognition. Understand the steps and components of attention mechanism architecture and see examples of its applications. Attention mechanism is a fundamental invention in artificial intelligence and machine learning, redefining the capabilities of deep learning models. This mechanism, inspired by the human mental process of selective focus, has emerged as a pillar in a variety of applications, accelerating developments in natural language processing, computer vision, and beyond. Attention Mechanisms address critical challenges in deep learning, including handling long sequences effectively, ensuring contextual understanding, and achieving improved overall model performance. In the Transformer series, we go over the ingredients that have made Transformers a universal recipe for machine learning. First up, we take a visual dive to understand the attention mechanism: Why transformers and attention took over. Developing a visual intuition behind Attention. Attention mechanisms represent advancements in machine learning and computer vision, enabling models to prioritize relevant information for better performance. As research progresses, attention mechanisms will further enhance the capabilities and interpretability of deep learning models.', name='duckduckgo_search', tool_call_id='call_1qtQMLUZLSeDBWOmFhM4xEvE')]\n",
            "\n",
            "\n",
            "\n",
            "Receiving update from node: 'agent'\n",
            "[AIMessage(content=\"### LoRA (Low-Rank Adaptation)\\n\\nLoRA, or Low-Rank Adaptation, is a machine learning technique designed to efficiently fine-tune large language models (LLMs) and other pretrained models. Instead of adjusting all the parameters of a model during fine-tuning, LoRA modifies only a small, low-rank subset of the model's parameters. This approach significantly reduces the computational resources required for fine-tuning, making it more memory-efficient. LoRA has been particularly impactful in adapting large models to specific tasks without the need for extensive computational power.\\n\\n### Tim Dettmers\\n\\nTim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that drastically reduces the GPU memory required to fine-tune large models, making it possible to fine-tune models with billions of parameters on a single GPU. Tim Dettmers is affiliated with the Allen Institute for AI (Ai2) and has made significant contributions to the democratization of AI through his research.\\n\\n### Attention Mechanism in Machine Learning\\n\\nThe attention mechanism is a fundamental concept in deep learning that allows models to focus on relevant parts of the input data, improving performance in various tasks such as machine translation, image captioning, and speech recognition. Inspired by the human cognitive process of selective focus, attention mechanisms enable models to handle long sequences effectively, ensure contextual understanding, and achieve better overall performance.\\n\\nAttention mechanisms are a key component of Transformer models, which have revolutionized the field of natural language processing and computer vision. By prioritizing relevant information, attention mechanisms enhance the capabilities and interpretability of deep learning models, making them more efficient and effective in handling complex tasks.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 379, 'prompt_tokens': 1128, 'total_tokens': 1507}, 'model_name': 'gpt-4o-2024-05-13', 'system_fingerprint': 'fp_157b3831f5', 'finish_reason': 'stop', 'logprobs': None}, id='run-74e9a1df-6bff-4821-b6f9-bd64178b1d41-0', usage_metadata={'input_tokens': 1128, 'output_tokens': 379, 'total_tokens': 1507})]\n",
            "\n",
            "\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "\n",
            "LoRA, or Low-Rank Adaptation, is a machine learning technique designed to efficiently fine-tune large language models (LLMs) and other pretrained models. Instead of adjusting all the parameters of a model during fine-tuning, LoRA modifies only a small, low-rank subset of the model's parameters. This approach significantly reduces the computational resources required for fine-tuning, making it more memory-efficient. LoRA has been particularly impactful in adapting large models to specific tasks without the need for extensive computational power.\n",
            "\n",
            "### Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a researcher specializing in efficient deep learning, particularly at the intersection of machine learning, natural language processing (NLP), and computer systems. He is known for his work on quantization and the use of deep learning hardware accelerators. One of his notable contributions is QLoRA, a method that drastically reduces the GPU memory required to fine-tune large models, making it possible to fine-tune models with billions of parameters on a single GPU. Tim Dettmers is affiliated with the Allen Institute for AI (Ai2) and has made significant contributions to the democratization of AI through his research.\n",
            "\n",
            "### Attention Mechanism in Machine Learning\n",
            "\n",
            "The attention mechanism is a fundamental concept in deep learning that allows models to focus on relevant parts of the input data, improving performance in various tasks such as machine translation, image captioning, and speech recognition. Inspired by the human cognitive process of selective focus, attention mechanisms enable models to handle long sequences effectively, ensure contextual understanding, and achieve better overall performance.\n",
            "\n",
            "Attention mechanisms are a key component of Transformer models, which have revolutionized the field of natural language processing and computer vision. By prioritizing relevant information, attention mechanisms enhance the capabilities and interpretability of deep learning models, making them more efficient and effective in handling complex tasks.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "async for chunk in agent_with_helpfulness_check.astream(inputs, stream_mode=\"updates\"):\n",
        "    for node, values in chunk.items():\n",
        "        print(f\"Receiving update from node: '{node}'\")\n",
        "        print(values[\"messages\"])\n",
        "        print(\"\\n\\n\")\n",
        "\n",
        "print(chunk['agent']['messages'][0].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "0593c6f5-0a1d-41f6-960b-f32d101b3ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prompt engineering is a concept primarily associated with the field of artificial intelligence, particularly in the context of natural language processing (NLP) and large language models like GPT-3. It involves the design and crafting of prompts (input text) to elicit desired responses from AI models. The goal is to optimize the input to get the most accurate, relevant, or useful output from the model.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing questions or statements that guide the AI to produce the desired type of response.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve the quality of the output.\n",
            "3. **Understanding Model Behavior**: Gaining insights into how the model interprets different types of input to better predict and control its responses.\n",
            "\n",
            "### When Did It Break Onto the Scene?\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The ability of these models to generate human-like text based on prompts led to a growing interest in how to effectively communicate with them to achieve specific goals. The term \"prompt engineering\" itself started gaining traction around this time as researchers and practitioners began to explore and document best practices for interacting with these advanced AI systems.\n",
            "\n",
            "Would you like more detailed information or recent developments in prompt engineering?\n",
            "\n",
            "\n",
            "\n",
            "RAG stands for Retrieval-Augmented Generation. It is a technique in natural language processing (NLP) that combines retrieval-based methods with generative models to improve the quality and relevance of generated text. The key idea is to retrieve relevant documents or pieces of information from a large corpus and use this retrieved information to guide the generation process.\n",
            "\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The approach leverages both the strengths of retrieval-based models, which can access a vast amount of information, and generative models, which can produce coherent and contextually appropriate text.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n",
            "\n",
            "\n",
            "\n",
            "Fine-tuning is a process in machine learning where a pre-trained model is further trained on a new dataset to adapt it to a specific task. This approach leverages the knowledge the model has already acquired during its initial training on a large dataset, making it more efficient and effective for specialized tasks. Fine-tuning is particularly popular in natural language processing (NLP) and computer vision.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: A model is initially trained on a large, general dataset.\n",
            "2. **Fine-tuning**: The pre-trained model is then trained on a smaller, task-specific dataset.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Requires less data and computational resources compared to training a model from scratch.\n",
            "- **Performance**: Often results in better performance on the specific task due to the model's prior knowledge.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of large pre-trained models like BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer). These models demonstrated that fine-tuning could achieve state-of-the-art results on a variety of NLP tasks.\n",
            "\n",
            "To pinpoint the exact timeline and key papers that popularized fine-tuning, I can look up relevant scientific articles and historical milestones. Would you like me to do that?\n",
            "\n",
            "\n",
            "\n",
            "LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These tasks can include natural language understanding, text generation, translation, summarization, question answering, and more. The core technology behind these agents is typically a deep learning model trained on vast amounts of text data, enabling it to understand and generate human-like text.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Processing (NLP):** They excel in understanding and generating human language.\n",
            "2. **Contextual Understanding:** They can maintain context over longer conversations or documents.\n",
            "3. **Versatility:** They can be fine-tuned for specific tasks or domains.\n",
            "4. **Scalability:** They can handle a wide range of applications from chatbots to complex decision-making systems.\n",
            "\n",
            "### Breakthrough and Evolution:\n",
            "The concept of LLM-based agents has been around for a while, but significant breakthroughs occurred with the development of models like OpenAI's GPT (Generative Pre-trained Transformer) series. Here are some key milestones:\n",
            "\n",
            "1. **2018 - GPT-1:** OpenAI released the first version of GPT, which demonstrated the potential of pre-trained language models.\n",
            "2. **2019 - GPT-2:** This version showed significant improvements in text generation and understanding, leading to widespread attention and adoption.\n",
            "3. **2020 - GPT-3:** With 175 billion parameters, GPT-3 became one of the largest and most powerful language models, capable of performing a wide range of tasks with minimal fine-tuning.\n",
            "4. **2021 and Beyond:** Continued advancements in model architecture, training techniques, and the release of other large models like Google's BERT, T5, and more have further solidified the role of LLM-based agents in AI applications.\n",
            "\n",
            "### Current Trends:\n",
            "- **Integration in Products:** LLM-based agents are now integrated into various products and services, including virtual assistants, customer service bots, and content creation tools.\n",
            "- **Research and Development:** Ongoing research aims to make these models more efficient, ethical, and capable of understanding more complex tasks.\n",
            "- **Ethical Considerations:** There is a growing focus on addressing biases, ensuring data privacy, and making these models more transparent and accountable.\n",
            "\n",
            "Would you like more detailed information on any specific aspect of LLM-based agents?\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print(messages[\"messages\"][-1].content)\n",
        "  print(\"\\n\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
